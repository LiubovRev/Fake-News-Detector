{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d4d9bc",
   "metadata": {},
   "source": [
    "## Fake News Classification Project v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27539b13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a64c5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers not available — BERT disabled.\n",
      "cuML detected — GPU acceleration enabled for classical ML.\n",
      "============================================================\n",
      "EXPLORATORY DATA ANALYSIS\n",
      "============================================================\n",
      "Dataset shape: (44680, 4)\n",
      "\n",
      "Class distribution:\n",
      " is_fake\n",
      "1    23469\n",
      "0    21211\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ADVANCED TEXT PREPROCESSING\n",
      "============================================================\n",
      "Cleaning text… (slow step)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1248606/4031079760.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cleaning text… (slow step)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_clean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvanced_preprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_clean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvanced_preprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4933\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4934\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4935\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4937\u001b[0m     def _reindex_indexer(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1503\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1248606/4031079760.py\u001b[0m in \u001b[0;36madvanced_preprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"http\\S+|www\\S+|https\\S+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\S+@\\S+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[^\\w\\s]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fake News Classification Project - GPU + tqdm version\n",
    "====================================================\n",
    "Complete ML pipeline with GPU acceleration (cuML + CUDA),\n",
    "tqdm progress bars, robust preprocessing, augmentation,\n",
    "cross-validation, and optional BERT fine-tuning.\n",
    "\"\"\"\n",
    "\n",
    "# =========================\n",
    "# 0. Imports\n",
    "# =========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Try PyTorch + Transformers\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    BERT_AVAILABLE = True\n",
    "except:\n",
    "    print(\"Transformers not available — BERT disabled.\")\n",
    "    BERT_AVAILABLE = False\n",
    "\n",
    "# Try RAPIDS cuML GPU acceleration\n",
    "USE_GPU_CLASSICAL = True\n",
    "try:\n",
    "    if USE_GPU_CLASSICAL:\n",
    "        import cuml\n",
    "        from cuml.feature_extraction.text import TfidfVectorizer as cuTFIDF\n",
    "        from cuml.linear_model import LogisticRegression as cuLR\n",
    "        from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "        import cupy as cp\n",
    "        CUML_AVAILABLE = True\n",
    "        print(\"cuML detected — GPU acceleration enabled for classical ML.\")\n",
    "    else:\n",
    "        CUML_AVAILABLE = False\n",
    "except Exception:\n",
    "    print(\"cuML NOT available — using CPU scikit-learn.\")\n",
    "    CUML_AVAILABLE = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLTK downloads\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# =========================\n",
    "# GPU selection (CUDA / MPS / CPU)\n",
    "# =========================\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA GPU\")\n",
    "        return torch.device(\"cuda\")\n",
    "    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        print(\"Using Apple MPS GPU\")\n",
    "        return torch.device(\"mps\")\n",
    "    print(\"Using CPU\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "# =========================\n",
    "# 1. Load & Explore Data\n",
    "# =========================\n",
    "print(\"=\"*60)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "CSV_PATH = '/home/liubov/Téléchargements/fake_news_full_data.csv'\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nClass distribution:\\n\", df['is_fake'].value_counts())\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.countplot(data=df, x='is_fake')\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# =========================\n",
    "# 2. Robust Advanced Text Preprocessing\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADVANCED TEXT PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def safe_pos_tag(words):\n",
    "    try:\n",
    "        return pos_tag(words)\n",
    "    except:\n",
    "        return [(w, 'N') for w in words]\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = safe_pos_tag([word])[0][1][0].upper()\n",
    "    return {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}.get(tag, wordnet.NOUN)\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = str(text).lower()\n",
    "\n",
    "    contractions = {\n",
    "        \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
    "        \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\",\n",
    "        \"'d\": \" would\", \"'m\": \" am\"\n",
    "    }\n",
    "    for c, e in contractions.items():\n",
    "        text = text.replace(c, e)\n",
    "\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    clean = []\n",
    "    for w in tokens:\n",
    "        if w not in stop_words and len(w) > 2:\n",
    "            clean.append(lemmatizer.lemmatize(w, get_wordnet_pos(w)))\n",
    "    return \" \".join(clean)\n",
    "\n",
    "print(\"Cleaning text… (slow step)\")\n",
    "df['text_clean'] = df['text'].apply(advanced_preprocess)\n",
    "df['title_clean'] = df['title'].apply(advanced_preprocess)\n",
    "\n",
    "# =========================\n",
    "# 3. Data Augmentation\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA AUGMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def synonym_replace(text, n=2):\n",
    "    words = text.split()\n",
    "    candidates = [w for w in words if wordnet.synsets(w)]\n",
    "    if not candidates: return text\n",
    "    random.shuffle(candidates)\n",
    "    for w in candidates[:n]:\n",
    "        syn = wordnet.synsets(w)[0].lemmas()[0].name()\n",
    "        words = [syn if x == w else x for x in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def random_delete(text, p=0.1):\n",
    "    words = text.split()\n",
    "    if len(words) <= 1: return text\n",
    "    kept = [w for w in words if random.random() > p]\n",
    "    return \" \".join(kept) if kept else random.choice(words)\n",
    "\n",
    "def random_swap(text, n=2):\n",
    "    words = text.split()\n",
    "    if len(words) < 2: return text\n",
    "    words = words[:]\n",
    "    for _ in range(n):\n",
    "        a, b = random.sample(range(len(words)), 2)\n",
    "        words[a], words[b] = words[b], words[a]\n",
    "    return \" \".join(words)\n",
    "\n",
    "aug_funcs = [synonym_replace, random_delete, random_swap]\n",
    "\n",
    "minority_class = df['is_fake'].value_counts().idxmin()\n",
    "minority_df = df[df['is_fake'] == minority_class]\n",
    "\n",
    "augmented = []\n",
    "for _, row in minority_df.sample(min(500, len(minority_df))).iterrows():\n",
    "    f = random.choice(aug_funcs)\n",
    "    new_text = f(row['text_clean'])\n",
    "    augmented.append({\n",
    "        'combined_text': row['title_clean'] + \" \" + new_text,\n",
    "        'is_fake': row['is_fake']\n",
    "    })\n",
    "\n",
    "aug_df = pd.DataFrame(augmented)\n",
    "\n",
    "# =========================\n",
    "# 4. Combine Features\n",
    "# =========================\n",
    "df['combined_text'] = df['title_clean'] + \" \" + df['text_clean']\n",
    "df_combined = pd.concat([\n",
    "    df[['combined_text', 'is_fake']],\n",
    "    aug_df[['combined_text', 'is_fake']]\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"Dataset after augmentation:\", df_combined.shape)\n",
    "\n",
    "# =========================\n",
    "# 5. Train-test split\n",
    "# =========================\n",
    "X = df_combined['combined_text']\n",
    "y = df_combined['is_fake']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 6. Baseline\n",
    "# =========================\n",
    "baseline = y_train.mode()[0]\n",
    "y_pred_base = np.full_like(y_test, baseline)\n",
    "baseline_f1 = f1_score(y_test, y_pred_base)\n",
    "baseline_acc = accuracy_score(y_test, y_pred_base)\n",
    "\n",
    "# =========================\n",
    "# 7. Logistic Regression (GPU/CPU Auto)\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 2: LOGISTIC REGRESSION (GPU/CPU AUTO)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if CUML_AVAILABLE:\n",
    "    print(\"Using cuML GPU TF-IDF + Logistic Regression\")\n",
    "    tfidf = cuTFIDF(max_features=5000, ngram_range=(1, 3))\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "    lr_model = cuLR(max_iter=1000)\n",
    "    lr_model.fit(X_train_tfidf, y_train.astype(\"int32\"))\n",
    "\n",
    "    y_pred_lr = lr_model.predict(X_test_tfidf).get()\n",
    "    lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "    lr_acc = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "else:\n",
    "    print(\"Using CPU TF-IDF + GridSearchCV Logistic Regression\")\n",
    "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "    lr = GridSearchCV(\n",
    "        estimator=LogisticRegression(max_iter=1000),\n",
    "        param_grid={\n",
    "            'C': [0.1, 1, 10],\n",
    "            'penalty': ['l2'],\n",
    "            'solver': ['liblinear', 'saga']\n",
    "        },\n",
    "        scoring='f1',\n",
    "        cv=3\n",
    "    )\n",
    "    lr.fit(X_train_tfidf, y_train)\n",
    "    y_pred_lr = lr.predict(X_test_tfidf)\n",
    "\n",
    "    lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "    lr_acc = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "# =========================\n",
    "# 8. Random Forest (GPU/CPU Auto)\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 3: RANDOM FOREST (GPU/CPU AUTO)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if CUML_AVAILABLE:\n",
    "    print(\"Using cuML GPU Random Forest\")\n",
    "    cv = CountVectorizer(max_features=5000)\n",
    "    X_train_cv = cv.fit_transform(X_train)\n",
    "    X_test_cv = cv.transform(X_test)\n",
    "\n",
    "    X_train_gpu = cp.sparse.csr_matrix(X_train_cv)\n",
    "    X_test_gpu = cp.sparse.csr_matrix(X_test_cv)\n",
    "\n",
    "    rf_model = cuRF(n_estimators=200, max_depth=30)\n",
    "    rf_model.fit(X_train_gpu, y_train.astype(\"int32\"))\n",
    "\n",
    "    y_pred_rf = rf_model.predict(X_test_gpu).get()\n",
    "    rf_f1 = f1_score(y_test, y_pred_rf)\n",
    "    rf_acc = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "else:\n",
    "    print(\"Using CPU Random Forest\")\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=30, class_weight=\"balanced\")\n",
    "    rf.fit(X_train_cv, y_train)\n",
    "    y_pred_rf = rf.predict(X_test_cv)\n",
    "    rf_f1 = f1_score(y_test, y_pred_rf)\n",
    "    rf_acc = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# =========================\n",
    "# 9–10: Gradient Boosting + Ensemble (CPU)\n",
    "# =========================\n",
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train_tfidf, y_train)\n",
    "y_pred_gb = gb.predict(X_test_tfidf)\n",
    "gb_f1 = f1_score(y_test, y_pred_gb)\n",
    "gb_acc = accuracy_score(y_test, y_pred_gb)\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', lr_model if CUML_AVAILABLE else lr.best_estimator_),\n",
    "        ('gb', gb)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "ensemble.fit(X_train_tfidf, y_train)\n",
    "y_pred_ensemble = ensemble.predict(X_test_tfidf)\n",
    "ensemble_f1 = f1_score(y_test, y_pred_ensemble)\n",
    "ensemble_acc = accuracy_score(y_test, y_pred_ensemble)\n",
    "\n",
    "# =========================\n",
    "# 11. tqdm-Based BERT Training\n",
    "# =========================\n",
    "if BERT_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL 6: BERT FINE-TUNING\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    class NewsDataset(Dataset):\n",
    "        def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __len__(self): return len(self.texts)\n",
    "        def __getitem__(self, idx):\n",
    "            enc = self.tokenizer.encode_plus(\n",
    "                self.texts.iloc[idx],\n",
    "                max_length=self.max_len,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': enc['input_ids'].squeeze(),\n",
    "                'attention_mask': enc['attention_mask'].squeeze(),\n",
    "                'labels': torch.tensor(self.labels.iloc[idx])\n",
    "            }\n",
    "\n",
    "    N = 1000\n",
    "    X_train_b = X_train.head(N).reset_index(drop=True)\n",
    "    y_train_b = y_train.head(N).reset_index(drop=True)\n",
    "    X_test_b = X_test.head(200).reset_index(drop=True)\n",
    "    y_test_b = y_test.head(200).reset_index(drop=True)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "    train_loader = DataLoader(NewsDataset(X_train_b, y_train_b, tokenizer), batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(NewsDataset(X_test_b, y_test_b, tokenizer), batch_size=16)\n",
    "\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    epochs = 2\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in loop:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            ms = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            out = model(ids, attention_mask=ms, labels=labels)\n",
    "            loss = out.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loop.set_postfix(loss=float(loss))\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    truth = []\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating BERT\"):\n",
    "        ids = batch['input_ids'].to(device)\n",
    "        ms = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(ids, attention_mask=ms).logits\n",
    "\n",
    "        p = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
    "        preds.extend(p)\n",
    "        truth.extend(labels.numpy().tolist())\n",
    "\n",
    "    bert_f1 = f1_score(truth, preds)\n",
    "    bert_acc = accuracy_score(truth, preds)\n",
    "else:\n",
    "    bert_f1 = 0\n",
    "    bert_acc = 0\n",
    "\n",
    "# =========================\n",
    "# 12. Results Table\n",
    "# =========================\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\n",
    "        \"Baseline\",\n",
    "        \"Logistic Regression\",\n",
    "        \"Random Forest\",\n",
    "        \"Gradient Boosting\",\n",
    "        \"Ensemble\",\n",
    "        \"BERT\"\n",
    "    ],\n",
    "    \"F1-Score\": [\n",
    "        baseline_f1, lr_f1, rf_f1, gb_f1, ensemble_f1, bert_f1\n",
    "    ],\n",
    "    \"Accuracy\": [\n",
    "        baseline_acc, lr_acc, rf_acc, gb_acc, ensemble_acc, bert_acc\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nFINAL RESULTS:\\n\", results)\n",
    "\n",
    "# =========================\n",
    "# 13. Confusion matrix (best model)\n",
    "# =========================\n",
    "best_model = results.iloc[results['F1-Score'].idxmax()]\n",
    "print(\"\\nBest model:\", best_model['Model'])\n",
    "\n",
    "if best_model['Model'] == \"Logistic Regression\":\n",
    "    y_pred_best = y_pred_lr\n",
    "elif best_model['Model'] == \"Random Forest\":\n",
    "    y_pred_best = y_pred_rf\n",
    "elif best_model['Model'] == \"Gradient Boosting\":\n",
    "    y_pred_best = y_pred_gb\n",
    "elif best_model['Model'] == \"Ensemble\":\n",
    "    y_pred_best = y_pred_ensemble\n",
    "else:\n",
    "    y_pred_best = preds\n",
    "    y_test_eval = truth\n",
    "\n",
    "cm = confusion_matrix(y_test if best_model['Model'] != \"BERT\" else y_test_eval, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "plt.title(f\"Confusion Matrix - {best_model['Model']}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nProject finished successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb1d405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
